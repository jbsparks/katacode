{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building Container Images for High-Performance Computing\n",
    "---\n",
    "\n",
    "This workshop covers building container images with [Docker](https://www.docker.com) and [Singularity](https://www.sylabs.io/singularity).\n",
    "\n",
    "Among the topics covered are container specification files, the basics of building container images, and techniques for managing the size of container images.\n",
    "\n",
    "The lab assumes you are familiar with basic Linux shell commands.\n",
    "\n",
    "Before beginning, please make sure the lab environment is correctly setup by running the two cells below.  To run a cell, highlight the cell and press control-enter or click on the \"Run\" button in the toolbar.\n",
    "\n",
    "### 1.0.1 First lets find out which _Enterprise_ OCI runtime do we have, HPCaaS nodes may run Docker or RedHat podman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.2 Which singularity version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Why Containers for HPC?\n",
    "\n",
    "Container services and applications is an established technology the _Enterprise_, but this also applies to High-Performance Computing (HPC) and general data science.\n",
    "\n",
    "HPC native applications are often bound to a host environment.  For instance, building an application for a given system and then trying to run that binary on a different HPC system may not work.  Software dependencies such as MPI and math libraries are likely installed in different locations, may be different versions, or missing entirely.  The underlying Linux distribution may not even be the same.\n",
    "\n",
    "Containers bundle the entire application user-space into a single portable package.  As a result, the application environment is both portable and consistent, agnostic to the underlying system software configuration.  The container images may be deployed widely, and even shared with others, with confidence that the results will be reproducible regardless of the underlying system.\n",
    "\n",
    "Containers make life simple for both system administrators and end users.  System administrators do not need to maintain the hundreds of interdependent software packages requested by end users.  End users can download a container from a public repository such as the  [Docker Hub](https://hub.docker.com), or [Singularity Hub](https://singularity-hub.org) or private repository, and be running in a matter of minutes rather than the often lengthy process of building software for each specific system.\n",
    "\n",
    "Downloading and using a container image from a repository is the ideal case.  But what if the application environment of interest is not available?  This course will describe how you can build your own container images from scratch.  After you have successfully built an application container image, consider uploading it to a container repository so that others can benefit from your work.\n",
    "\n",
    "At the end of the course, you will build a container image for a real GPU enabled application.  It can take some time to build the application image of a typical HPC application code.  In order to speed the build up when we get to that point, we need to prefetch some of the software components in the background.  Don't worry about what it's doing right now, although by the end of the course you should understand what it is doing.  Go ahead to the next section after evaluating the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull ubuntu:16.04\n",
    "!docker pull centos:7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to wait until the above has completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Container Image Formats\n",
    "\n",
    "The [Singularity container runtime](https://www.sylabs.io/singularity/) is specifically designed for the High-Performance Computing use case.  Besides features such as running containers without requiring access to a superuser account, the Singularity container image format is a single \"flat\" file.  That makes Singularity container images very easy to transfer between systems and share across a cluster.\n",
    "\n",
    "So why does this lab also cover building container images with Docker?  In short, while Singularity has many advantages as a container runtime for HPC, the Docker image builder has many advantages as a container image builder.  The Docker container image is [\"layered\"](https://github.com/opencontainers/image-spec).  The advantages of \"layered\" images include a build cache to speed up building container images and multi-stage builds to minimize the size of the final container image by more precisely controlling the image content.\n",
    "\n",
    "Fortunately, Singularity can easily work with Docker images.  The best practice described in this lab for HPC containers is:\n",
    "\n",
    "1. Build container images with Docker\n",
    "2. Convert the Docker images to Singularity images\n",
    "3. Use Singularity to run containers on your HPC system\n",
    "\n",
    "This lab will cover all four of these topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Building Container Images With Singularity\n",
    "\n",
    "This part of the course covers how to [build container images with Singularity](https://sylabs.io/guides/3.2/user-guide/build_a_container.html).\n",
    "\n",
    "Administrative privileges are required to build Singularity container images.  In contrast to Docker, running Singularity containers does not require administrative privileges.  By default, Singularity uses a `setuid` helper program when elevated privileges are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Building Your First Singularity Image: Hello World!\n",
    "\n",
    "A [Singularity definition file](https://sylabs.io/guides/3.7/user-guide/definition_files.html) is a plain text file that specifies the instructions to create your container image. By convention this file is named `Singularity.def`, but any name may be used. The definition file syntax resembles the syntax of RPM spec files.\n",
    "\n",
    "For this first image, we'll use a very simple [definition file](Singularity.def) to build a container for the classic [\"Hello World!\" program](hello.c). Singularity will build your container image based on the ubuntu:16.04 container image from Docker Hub. It will try to find it locally first, then will go the default repository (Docker Hub) to download the image. \n",
    "\n",
    "The Ubuntu base container on Docker Hub does not include development tools in order to help minimize the size of the image. The definition file installs the __GNU C compiler__ and __standard C headers__. \n",
    "\n",
    "Once the development environment is setup, the \"Hello World\" program can be built from source.\n",
    "\n",
    "Build the \"Hello World\" container image by invoking `singularity build` with the definition file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo singularity build --force hello-world.sif ./Singularity.def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note of the `singularity build` command line. The first argument is the filename of the resulting container image. Singularity 3.x container images have the `.sif` extension. The second argument is the path to the Singularity definition file. \n",
    "\n",
    "The output `Build complete: hello-world.sif` indicates that the image was built successfully.\n",
    "\n",
    "Run the containerized \"Hello World\" program by invoking `singularity exec`.  Note that `sudo` is not required to use the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity exec hello-world.sif /usr/local/bin/hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World program run inside the container produces the expected output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the Hello World container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh hello-world.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World program itself is less than 10 kilobytes, yet the Hello World container image is 93 megabytes!  This is 2.5 times the size of the base Ubuntu 16.04 image (36 megabytes). The compiler accounts for over half of the total container size! But all we really care about is the Hello World program, there is no need to redistribute the compiler (or our source code) to users of the container image.\n",
    "\n",
    "You could reduce the size of the Singularity container image by [removing the source code and compiler](/lab/edit/singularity/Singularity.def.cleanup) after the Hello World program has been built.  Doing so would reduce the container image size to 36 megabytes. However, more complex programs with runtime dependencies would require more sophisticated cleanup steps to remove unnecessary components while maintaining the needed runtime dependencies.\n",
    "\n",
    "The Docker image format and build process includes capabilities that help control container image size and more precisely control the content of container images.\n",
    "\n",
    "### 1.3.2 Singularity Summary\n",
    "\n",
    "The content of Singularity container images is specified in Singularity definition files.\n",
    "\n",
    "Singularity container images are \"flat\", not layered like Docker (OCI) images.  Since flat container images are simple files, they are easy to copy and move.  However, building flat container images cannot take advantage of some features available with \"layered\" images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Building Container Images With Docker\n",
    "\n",
    "This part of the lab covers how to [build container images with Docker](https://docs.docker.com/engine/reference/commandline/build/).\n",
    "\n",
    "### 2.0.1 Building Your First Docker Image\n",
    "\n",
    "A [Dockerfile is a plain text file](https://docs.docker.com/engine/reference/builder/) that specifies the instructions to create your container image.  For this first image, we'll use a very simple [Dockerfile](Dockerfile.first).  Docker will build your container image based on the `ubuntu:16.04` container image from Docker Hub.  It will try to find it locally first, then will go the default repository (Docker Hub) to download the image.  After that is a `RUN` instruction that tells the container builder to run the shell command `date > /build-info.txt` and save the result as part of the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t first-image -f Dockerfile.first ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick note of the `docker build` command line.  The `-t` option specifies the name and tag of the resulting container image, with the name and tag separated by a colon. By default, Docker uses `latest` as the tag unless one is specified.  The `-f` option specifies the Dockerfile to build the container from.  And finally, the `.` is the path to use as the build context, i.e., the sandbox where files from the host are accessible during the container image build.\n",
    "\n",
    "The output `Successfully tagged first-image:latest` indicates that the image was built successfully. \n",
    "\n",
    "Note that each instruction from the Dockerfile is shown as a \"Step\".  As it builds the container image, Docker tells you which step it is on and gives the intermediate hash of the resulting layer.\n",
    "\n",
    "Let's check out the newly built image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm -it first-image cat /build-info.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date shown should be just a short time ago when you build the image.  The date in this file corresponds to when the container image was built, not when it is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Image Layering\n",
    "\n",
    "One of the most important concepts when building container images is *layering*.  Docker builds container images according to the [Open Container Initiative (OCI) image specification](https://github.com/opencontainers/image-spec).  OCI container images are composed of a series of layers. (If you look closely at the output of building the first container image above, you will see that the `ubuntu:16.04` container image itself actually consists of multiple layers.) The layers are applied sequentially, one on top of another, to form the container image that you ultimately see when running a container.\n",
    "\n",
    "To help illustrate layering, let's [extend the previous Dockerfile](/lab/edit/docker/Dockerfile.second) to add a second `RUN` instruction that appends the Linux kernel version of the system where the container was built to `/build-info.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM ubuntu:16.04\n",
      "\n",
      "RUN date > /build-info.txt\n",
      "RUN uname -r >> /build-info.txt\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t second-image -f Dockerfile.second ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, note that first 2 steps were cached.  Docker recognizes that the first 2 instructions have previously been processed, so the corresponding layers do not need to be regenerated.  This is possible due to layering.  The layer cache can significantly speed up building container images.  Recall that the layers are applied sequentially; so the entire history of instructions up to that point must be identical for the cached layer to be used.\n",
    "\n",
    "The third step which we just added to the Dockerfile is not in the cache, so it needs to be performed and a new layer is generated.\n",
    "\n",
    "Let's verify that the kernel version is included in the build info file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm -it second-image cat /build-info.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker provides a method to take a closer look at the layers composing a container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker history second-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your image consists of 7 layers.  The layers are listed in reverse chronological order; the container image you see when running the container is generated by starting from the last layer shown, applying the second to the last layer on top of it, then the third from the last on top of that, and so on. In case of conflicts, a subsequent layer will overwrite content from previous layers.\n",
    "\n",
    "The first column shows the layer hash. You can correlate the layer hashes shown here with the `docker build` output above.\n",
    "\n",
    "The second column shows when the layer was created.  You created the top 2 layers just a few minutes ago, while the other layers correspond to the `ubuntu:16.04` base image and were created longer ago.\n",
    "\n",
    "The third column shows an abbreviated version of the Dockerfile instruction used to build the corresponding layer.  To see the full instruction, use `docker history --no-trunc`.  The instructions for the top 2 layers match what was specified in the [Dockerfile](/lab/edit/docker/Dockerfile.second).\n",
    "\n",
    "The fourth column shows the size of the layer.  Why is the layer that appended the kernel version (`uname -r ...`) almost twice as large the layer that saved the date?  \n",
    "\n",
    "The OCI image specification employs file level deduplication to handle conflicts.  When a build instruction creates or modifies a file, the entire file is saved in the corresponding layer.  So when the kernel version was appended to the build info file, that layer did not capture just the difference, but rather the whole modified file.  In this particular case, the file is tiny and the amount of duplicated data is minimal.  But consider the case of a large, 1 GB file.  If a subsequent layer modifies a single byte in that file, the file will account for 2 GB in the container image, even though the file will appear to be \"only\" 1 GB when running the container.\n",
    "\n",
    "A best practice arising from file level deduplication of layers is to put all actions modifying the same set of files in the same Dockerfile instruction.  For example, remove any temporary files in the same instruction in which they are created.\n",
    "\n",
    "Let's modify the Dockerfile so that the [date and kernel version are written to the build info file in the same instruction](/lab/edit/docker/Dockerfile.third).  In the bash shell, commands can be concatenated with `&&`. (You may have noticed long `RUN` commands connected with `&&` in other Dockerfiles; this is why.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t third-image -f Dockerfile.third ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker history third-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$(docker history second-image > i2) $(docker history third-image > i3) sdiff i2 i3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there is now a single layer for the build info file and the extraneous layer with the duplicated data has been eliminated.\n",
    "\n",
    "Strike a balance between using lots of individual Dockerfile instructions versus using a single instruction.  Lots of individual instructions may produce unnecessarily large container images when touching the same files, but using too few instructions will eliminate the advantages of the build cache to speed up your container builds.  \n",
    "\n",
    "A best practice is to bundle all *related* items into a single layer, but to put unrelated items in separate layers.  For example, install the compiler in one layer and build your source code in another layer (but cleanup any temporary object files in the same layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.3 Hello World\n",
    "\n",
    "Let's put these techniques into practice by constructing a container image for the classic [\"Hello World!\" program](/lab/edit/sources/hello.c).\n",
    "\n",
    "#### 2.0.3.1 Exercise\n",
    "\n",
    "The Ubuntu base container on Docker Hub does not include development tools in order to help minimize the size of the image.  As an exercise, modify the [Dockerfile](/lab/edit/docker/Dockerfile.hello_exercise) to install the GNU C compiler and standard C headers.  For Ubuntu, the command to install packages is `apt-get`.  The packages are named `gcc` and `build-essential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t hello-world:exercise -f Dockerfile.exercise ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your solution by running the Hello World program inside the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm -it hello-world:exercise /usr/local/bin/hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0.3.2 Solution\n",
    "\n",
    "If you get stuck, or just want compare your approach, please see the [solution](Dockerfile.solution).\n",
    "\n",
    "Note that the apt package cache is removed in the same step where it is generated, following the recommended best practice of cleaning up temporary and unnecessary files in the same instruction where they are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: FROM ubuntu:16.04\n",
      "STEP 2: RUN apt-get update -y && apt-get install -y --no-install-recommends         build-essential         gcc\n",
      "--> Using cache c280b41aeeb3d69ec979874cd574a2fa79de99de745e30692d61020fb683d216\n",
      "STEP 3: COPY hello.c /var/tmp/hello.c\n",
      "--> Using cache 0967b8f7fd4ba90651da9d325469386ab8dec6292ee9c8297242bb038eb71337\n",
      "STEP 4: RUN gcc -o /usr/local/bin/hello /var/tmp/hello.c\n",
      "--> Using cache 1690367418fbdee7b267b6ba3fad5565fa8cbd77928ad500e9bfbf7ab713d1d1\n",
      "STEP 5: COMMIT hello-world:solution\n",
      "--> 1690367418f\n",
      "1690367418fbdee7b267b6ba3fad5565fa8cbd77928ad500e9bfbf7ab713d1d1\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t hello-world:solution -f Dockerfile.solution ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World program run inside the container produces the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm -it hello-world:solution /usr/local/bin/hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the layers in the Hello World container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker history hello-world:solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hello World program itself is less than 10 kilobytes, but the compiler and related tools are ~175 megabytes.  The compiler accounts for *over half* of the total container size!  But all we really care about is the Hello World program, there is no need to redistribute the compiler (or our source code) to users of the container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.4 Multi-Stage Hello World\n",
    "\n",
    "Docker [multi-stage builds](https://docs.docker.com/develop/develop-images/multistage-build/) are a way to control the size of container images.  In the same Dockerfile, you can define a second stage that is a completely separate container image and copy just the binary and any runtime dependencies from preceding stages into the image.  The output of a multi-stage build is a single container image corresponding to the last stage of the Dockerfile.  The multi-stage Hello World [Dockerfile](Dockerfile.hello_multistage) shows how a second `FROM` instruction starts a second stage, but where artifacts from the preceding stage can still be accessed (`COPY --from`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t hello-world:multistage -f Dockerfile.hello_multistage ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker history hello-world:multistage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run --rm -it hello-world:multistage /usr/local/bin/hello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The container image generated by the multi-stage build adds only the Hello World program to the base `ubuntu:16.04` image, yielding a significant savings in the size of the container.  Multi-stage builds can also be used to avoid redistributing source code or other build artifacts.  However, keep in mind this is a simple case and more complex cases may have additional runtime dependencies that also need to be copied from one stage to another.  HPC Container Maker can help ensure the necessary runtime dependencies are available in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker images hello-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.5 Docker Summary\n",
    "\n",
    "The content of Docker container images is specified in Dockerfiles.\n",
    "\n",
    "Docker (OCI) container images are layered.  Layering provides a number of advantages, including caching that can speed up builds and reducing disk usage when layers can be shared by several images.  However, layering also requires careful use to avoid pitfalls that can bloat the image size.\n",
    "\n",
    "Multi-stage builds are a very useful feature for fine tuning the content of container images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 HPC Containers\n",
    "\n",
    "### 3.0.1 HPC Containers, getting started\n",
    "\n",
    "These building blocks encapsulate the best practices of building HPC software components with the best practices of building container images to generate optimal container image specifications.  This lets you easily take advantage of all the existing knowledge of how to best install a component like OpenMPI inside a container image.\n",
    "\n",
    "To illustrate this, let's start with a simple [example](Dockerfile.openmpi_cuda) of a container image that includes CUDA and OpenMPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM nvidia/cuda:9.2-devel-centos7\n",
      "\n",
      "# OpenMPI version 4.0.1\n",
      "RUN yum install -y \\\n",
      "        bzip2 \\\n",
      "        file \\\n",
      "        hwloc \\\n",
      "        make \\\n",
      "        numactl-devel \\\n",
      "        openssh-clients \\\n",
      "        perl \\\n",
      "        tar \\\n",
      "        wget && \\\n",
      "    rm -rf /var/cache/yum/*\n",
      "RUN mkdir -p /var/tmp && wget -q -nc --no-check-certificate -P /var/tmp https://www.open-mpi.org/software/ompi/v4.0/downloads/openmpi-4.0.1.tar.bz2 && \\\n",
      "    mkdir -p /var/tmp && tar -x -f /var/tmp/openmpi-4.0.1.tar.bz2 -C /var/tmp -j && \\\n",
      "    cd /var/tmp/openmpi-4.0.1 &&   ./configure --prefix=/usr/local/openmpi --disable-getpwuid --enable-orterun-prefix-by-default --with-cuda --without-verbs && \\\n",
      "    make -j$(nproc) && \\\n",
      "    make -j$(nproc) install && \\\n",
      "    rm -rf /var/tmp/openmpi-4.0.1.tar.bz2 /var/tmp/openmpi-4.0.1\n",
      "ENV LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH \\\n",
      "    PATH=/usr/local/openmpi/bin:$PATH\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile.openmpi_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this simple two line recipe is processed by HPCCM, the optimized Dockerfile is generated.  Notice that the Dockerfile best practices described earlier, such as combining related steps into a single layer and removing temporary files in the same layer they are generated are automatically employed.\n",
    "\n",
    "A Singularity definition file can be generated from the exact same recipe just by specifying the `--format` command line option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat openmpi_cuda.def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we illustrate the same openmpi/cuda container but using __ubuntu__ rather than  __centos.7__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.openmpi_cuda_ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.2 Reproducing a Bare Metal Environment\n",
    "\n",
    "Many HPC systems use [environment modules](https://en.wikipedia.org/wiki/Environment_Modules_(software)) to manage their software environment. A user loads the modules corresponding to the desired software environment.\n",
    "\n",
    "```\n",
    "$ module load cuda/9.0\n",
    "$ module load gcc\n",
    "$ module load openmpi/1.10.7\n",
    "```\n",
    "\n",
    "Modules can depend on each other, and in this case, the openmpi module was built with the gcc compiler and with CUDA support enabled.\n",
    "\n",
    "The Linux distribution and drivers are typically fixed by the system administrator, for instance CentOS 7 and Mellanox OFED 3.4.\n",
    "\n",
    "The system administrator of the HPC system built and installed these components for their user community. Including a software component in a container image requires knowing how to properly configure and build the component. This is specialized knowledge and can be further complicated when applying container best practices.\n",
    "\n",
    "_How can this software environment be reproduced in a container image?_\n",
    "\n",
    "The starting point for any container image is a base image. Since CUDA is required, the base image should be one of the [publicly available CUDA base images](https://hub.docker.com/r/nvidia/cuda/). The CUDA base image corresponding to CUDA 9.0 and CentOS 7 is `nvidia/cuda:9.0-devel-centos7`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.cuda_gcc_openmpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for singularity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat cuda_gcc_openmpi.def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the desired workflow, the next step might be to use a text editor to add the steps to build an HPC application to the Dockerfile or Singularity definition file, or it might be to extend the HPCCM recipe to add the steps to build an HPC application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0.2.1 Exercises\n",
    "\n",
    "1. Modify [cuda-gcc-openmpi.py](/lab/edit/hpccm/cuda-gcc-openmpi.py) to use version 7 of the GNU compiler.  Refer to the [gnu building block](https://github.com/NVIDIA/hpc-container-maker/blob/master/docs/building_blocks.md#gnu) documentation for details.\n",
    "\n",
    "2. Modify [cuda-gcc-openmpi.py](/lab/edit/hpccm/cuda-gcc-openmpi.py) to use the PGI compilers. Change `compiler = gnu()` to `compiler = pgi(eula=True)`. Note: The PGI compiler EULA must be accepted in order to use the [PGI building block](https://github.com/NVIDIA/hpc-container-maker/blob/master/docs/building_blocks.md#pgi).\n",
    "\n",
    "3. Modify [cuda-gcc-openmpi.py](/lab/edit/hpccm/cuda-gcc-openmpi.py) so that the Linux distribution is Ubuntu instead of CentOS.  Modify the base image from `nvidia/cuda:9.0-devel-centos7` to `nvidia/cuda:9.0-devel-ubuntu16.04`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.3 MPI Bandwidth\n",
    "\n",
    "The [MPI Bandwidth sample program](mpi_bandwidth.c) from the Lawrence Livermore National Laboratory (LLNL) will be used as a proxy application to illustrate to build mpi programs.\n",
    "\n",
    "The CentOS 7 base image is sufficient for this example. The Mellanox OFED user space libraries, a compiler, and MPI library are also needed. For this tutorial section, the GNU compiler and OpenMPI will be used.\n",
    "\n",
    "The next step is to build the MPI Bandwidth program from source. First the source code must be copied into the container, and then compiled. \n",
    "Finally, compile the program binary using the mpicc MPI compiler wrapper.\n",
    "\n",
    "```bash\n",
    "    mpicc -o /usr/local/bin/mpi_bandwidth /var/tmp/mpi_bandwidth.c\n",
    "```\n",
    "\n",
    "Note: In a production container image, a cleanup step would typically also be performed to remove the source code and any other build artifacts. That step is skipped here. [Multi-stage Docker builds](https://docs.docker.com/develop/develop-images/multistage-build/) are another approach that separates the application build process from the application deployment.\n",
    "\n",
    "\n",
    "To run MPI Bandwidth from a container, first generate the Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.mpi_bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, create the Docker container image.  The cell immediately below will load prebuilt (cached) versions of the Docker image layers up to and including OpenMPI to significantly reduce the container image build time.  This is not strictly required, but the MPI Bandwidth container image will take 10-15 minutes to build if the cache is not loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker load -i cache/mpi_bandwidth_cache.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.mpi_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t mpi_bandwidth -f Dockerfile.mpi_bandwidth ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, convert the Docker container image to a Singularity container image.  The `docker-daemon` endpoint tells Singularity to use the local Docker image repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity build mpi_bandwidth.sif docker-daemon://mpi_bandwidth:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are running podman, the RedHat equivalent to docker. But podman is not implemented as a daemon, so we must do something a little different.\n",
    "\n",
    "* export the image\n",
    "* singularity build from the tar file\n",
    "\n",
    "Using this methor we typically build __OCI-first__, then covert to _.sif_ format as a way to establish better mobility with containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!podman save -o mpi_bandwidth.tar mpi_bandwidth:latest && ls -lh mpi_bandwidth.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity build --force mpi_bandwidth.sif docker-archive://mpi_bandwidth.tar && rm -f mpi_bandwidth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run MPI Bandwidth using Singularity with 2 MPI ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity exec mpi_bandwidth.sif mpirun --allow-run-as-root -n 2 -mca btl_base_warn_component_unused 0 /usr/local/bin/mpi_bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact same container images may also be used for multi-node runs, but that is beyond the scope of this lab. The webinar [Scaling Out HPC Workflows with NGC and Singularity](https://info.nvidia.com/simplfying-workflows-with-singularity-reg-page.html?ondemandrgt=yes) is a good reference for multi-node MPI runs.\n",
    "\n",
    "#### Exercises\n",
    "\n",
    "1. Modify [mpi_bandwidth.py](/lab/edit/hpccm/mpi_bandwidth.py) to use MVAPICH2 instead of OpenMPI.  Consult the [MVAPICH2 building block](https://github.com/NVIDIA/hpc-container-maker/blob/master/docs/building_blocks.md#mvapich2) documentation for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.4 Multi-stage Recipes\n",
    "\n",
    "[Multi-stage Docker builds](https://docs.docker.com/develop/develop-images/multistage-build/) are a very useful capability that separates the application build step from the deployment step. The development toolchain, application source code, and build artifacts are not necessary when deploying the built application inside a container. In fact, they can significantly and unnecessarily increase the size of the container image.\n",
    "\n",
    "The [Dockerfile.cuda_multistage](Dockerfile.cuda_multistage) recipe installs the GNU compiler in the first (build) stage, but only the corresponding runtime libraries in the second (deployment) stage. Building block settings defined in the first stage are automatically reflected in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.cuda_multistage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0.5 Multi-stage MPI Bandwidth\n",
    "\n",
    "By adding just a few more lines to the recipe, the MPI Bandwidth example can be improved from a [single stage recipe](/lab/edit/hpccm/mpi_bandwidth.py) to a [multi-stage recipe](/lab/edit/hpccm/mpi_bandwidth_multistage.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat Dockerfile.mpi_bandwidth_multistage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker build -t mpi_bandwidth:multistage -f Dockerfile.mpi_bandwidth_multistage ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, fi we are running podman, we need to do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity build mpi_bandwidth_multistage.sif docker-daemon://mpi_bandwidth:multistage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-stage container image functionality is the same, but the container image is smaller because the development environment is not being redistributed with the MPI Bandwidth workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!podman save -o mpi_bandwidth_multistage.tar mpi_bandwidth:multistage && ls -lh mpi_bandwidth_multistage.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity build --force mpi_bandwidth_multistage.sif docker-archive://mpi_bandwidth_multistage.tar && rm -f mpi_bandwidth_multistage.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity exec mpi_bandwidth_multistage.sif mpirun --allow-run-as-root -n 2 -mca btl_base_warn_component_unused 0 /usr/local/bin/mpi_bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh mpi_bandwidth*.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 miniWeather: A Simple Example Application\n",
    "\n",
    "The [miniWeather code](https://github.com/mrnorman/miniWeather) mimics the basic dynamics seen in atmospheric weather and climate. The dynamics themselves are dry compressible, stratified, non-hydrostatic flows dominated by buoyant forces that are relatively small perturbations on a hydrostatic background state.  The equations in this code themselves form the backbone of pretty much all fluid dynamics codes, and this particular flavor forms the base of all weather and climate modeling.\n",
    "\n",
    "With about 500 total lines of code (and only about 200 lines that you care about), it serves as an approachable place to learn parallelization and porting using MPI + X, where X is OpenMP, OpenACC, CUDA, or potentially other approaches to CPU and accelerated parallelization.\n",
    "\n",
    "A mini app simulating weather-like flows for training in parallelizing accelerated HPC architectures. Currently includes:\n",
    "\n",
    "* MPI (C, Fortran, and C++)\n",
    "* OpenACC Offload (C and Fortran)\n",
    "* OpenMP Threading (C and Fortran)\n",
    "* OpenMP Offload (C and Fortran)\n",
    "* C++ Portability\n",
    ">* CUDA-like approach\n",
    ">* https://github.com/mrnorman/YAKL/wiki/CPlusPlus-Performance-Portability-For-OpenACC-and-OpenMP-Folks\n",
    ">* C++ code works on CPU, Nvidia GPUs (CUDA), and AMD GPUs (HIP)\n",
    "\n",
    "Author: Matt Norman, Oak Ridge National Laboratory, https://mrnorman.github.io\n",
    "\n",
    "The miniWeather container image will take about 5 minutes to build, assuming the PGI compiler container image was prefetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: FROM ubuntu:18.04 AS build\n",
      "STEP 2: RUN apt-get update -y &&     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         m4 make tar build-essential cmake gcc gfortran wget &&     rm -rf /var/lib/apt/lists/*\n",
      "--> Using cache fcaeb9a68411b1c4f5b02a77dea9d8d6df453e326251169fc04bfad31381e31c\n",
      "STEP 3: RUN mkdir -p /var/tmp && wget -q -nc --no-check-certificate -P /var/tmp https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.7.tar.bz2 &&     mkdir -p /var/tmp && tar -x -f /var/tmp/openmpi-1.10.7.tar.bz2 -C /var/tmp -j &&     cd /var/tmp/openmpi-1.10.7 &&  CC=gcc CXX=g++ F77=gfortran F90=gfortran FC=gfortran ./configure --prefix=/usr/local/openmpi --disable-getpwuid --enable-orterun-prefix-by-default &&     make -j$(nproc) &&     make -j$(nproc) install &&     rm -rf /var/tmp/openmpi-1.10.7.tar.bz2 /var/tmp/openmpi-1.10.7\n",
      "--> Using cache 0d61b896a56e217660b77b740e4f8892c811752f4f7c755515936490564a88a5\n",
      "STEP 4: ENV LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH     PATH=/usr/local/openmpi/bin:$PATH\n",
      "--> Using cache 4f527757a9f3aa896c1b46feafb997413828f9c7e307eb775765df3e4a39df74\n",
      "STEP 5: RUN mkdir -p /var/tmp && wget -q -nc --no-check-certificate -P /var/tmp https://parallel-netcdf.github.io/Release/pnetcdf-1.11.2.tar.gz &&     mkdir -p /var/tmp && tar -x -f /var/tmp/pnetcdf-1.11.2.tar.gz -C /var/tmp -z &&     cd /var/tmp/pnetcdf-1.11.2 &&  CC=mpicc CXX=mpicxx F77=mpif77 F90=mpif90 FC=mpifort ./configure --prefix=/usr/local/pnetcdf --enable-shared &&     sed -i -e 's#pic_flag=\"\"#pic_flag=\" -fpic -DPIC\"#' -e 's#wl=\"\"#wl=\"-Wl,\"#' /var/tmp/pnetcdf-1.11.2/libtool &&     make -j$(nproc) &&     make -j$(nproc) install &&     rm -rf /var/tmp/pnetcdf-1.11.2.tar.gz /var/tmp/pnetcdf-1.11.2\n",
      "--> Using cache e738b6eacccacd840658bd81bc126face126f384451450b2f407d5caa843f470\n",
      "STEP 6: ENV LD_LIBRARY_PATH=/usr/local/pnetcdf/lib:$LD_LIBRARY_PATH     PATH=/usr/local/pnetcdf/bin:$PATH\n",
      "--> Using cache 6cf9146a1ce2213022598cbcf7db48b6591555544a0770f6870c5bb0ba747f07\n",
      "STEP 7: RUN apt-get update -y &&     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         ca-certificates         git &&     rm -rf /var/lib/apt/lists/*\n",
      "--> Using cache b24eda96dcc1166f586635afffa26652ba1d4c4e18e81de6bc0a400570a61784\n",
      "STEP 8: ENV PNETCDF_PATH=/usr/local/pnetcdf\n",
      "--> Using cache c74d2cab3dcd2337de9e8aea0756f54e2144e7ba052f5636ffa0e2dfd6af799b\n",
      "STEP 9: RUN mkdir -p /var/tmp && cd /var/tmp && git clone  https://github.com/mrnorman/miniWeather miniWeather && cd miniWeather && git submodule update --init && cd c/build && . ./cmake_fhqwhgads_gnu.sh &&     make &&     mkdir -p /opt/miniWeather &&     cd /var/tmp/miniWeather &&     install -m 755 -d /opt/miniWeather/bin &&     install -m 755 /var/tmp/miniWeather/c/build/mpi /opt/miniWeather/bin/miniWeather_mpi &&     install -m 755 /var/tmp/miniWeather/c/build/openmp /opt/miniWeather/bin/miniWeather_openmp &&     install -m 755 /var/tmp/miniWeather/c/build/serial /opt/miniWeather/bin/miniWeather_serial &&     rm -rf /var/tmp/miniWeather\n",
      "--> Using cache db661d79cfe912aae5e38b92df11994163d57570fc2ee08a59d628b580286282\n",
      "STEP 10: FROM ubuntu:18.04\n",
      "STEP 11: RUN apt-get update -y &&     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         libnuma1         openssh-client gfortran &&     rm -rf /var/lib/apt/lists/*\n",
      "--> Using cache 631494e6707eb87a9a346262c0ee70b3a0c1116caabb5ac189be6c5a5d28ac78\n",
      "STEP 12: RUN mkdir -p /usr/local/openmpi && mkdir -p /usr/local/pnetcdf\n",
      "--> Using cache 30591ae139423aa39507483b3bde7dd49bc74e5c3e4cee1c1e319539fa09d19d\n",
      "STEP 13: COPY --from=build /usr/local/openmpi /usr/local/openmpi \n",
      "--> Using cache 84d4e41ebdec0853f4aea8e01a2973dee8d22df341a0ae7297d635d4bd51f7c0\n",
      "STEP 14: ENV LD_LIBRARY_PATH=/usr/local/openmpi/lib:$LD_LIBRARY_PATH     PATH=/usr/local/openmpi/bin:$PATH\n",
      "--> Using cache 639450880b5ca37f6e2e57dfd06a3e093ef7ca8e6d3c7db0cc2fe42a68640257\n",
      "STEP 15: RUN apt-get update -y &&     DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends         libatomic1 &&     rm -rf /var/lib/apt/lists/*\n",
      "--> Using cache ebe1a5f473d903ed60cbee65c402276e15a8e55e95b3bf2b7bcbdcca425a3bc9\n",
      "STEP 16: COPY --from=build /usr/local/pnetcdf /usr/local/pnetcdf\n",
      "--> Using cache dce0f70ced255abc5e8b30baf044e2e1ba9d15a9aa0010891e2854a588b7fb7a\n",
      "STEP 17: ENV LD_LIBRARY_PATH=/usr/local/pnetcdf/lib:$LD_LIBRARY_PATH     PATH=/usr/local/pnetcdf/bin:$PATH\n",
      "--> Using cache f42e9e5f8d9a17bbd61e16c4a91ae6e3a8a9fa3b67e121d4665db95eb9422416\n",
      "STEP 18: COPY --from=build /opt/miniWeather /opt/miniWeather\n",
      "--> Using cache a716e399b1d50dc29bcb5285f8176edb8c45e4575cbe0455db48a09ee6a38e95\n",
      "STEP 19: ENV PATH=/opt/miniWeather/bin:$PATH\n",
      "--> Using cache c6dc7c9e3564746469c1e5fb9f8a89da8f4c5d2a51e77cd951e43a93b9fd34ea\n",
      "STEP 20: COMMIT miniweather\n",
      "--> c6dc7c9e356\n",
      "c6dc7c9e3564746469c1e5fb9f8a89da8f4c5d2a51e77cd951e43a93b9fd34ea\n"
     ]
    }
   ],
   "source": [
    "!sudo docker build -t miniweather -f Dockerfile.miniweather ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The miniWeather recipe uses a multi-stage build to reduce the Docker container image size from approximately 9 gigabytes to about 500 megabytes.  After conversion to Singularity, the final container image size is about 120 megabytes.\n",
    "\n",
    "Third, convert the Docker container image into a Singularity container image. This allows Singularity to (indirectly) take advantage of multi-stage builds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!podman save -o miniweather.tar miniweather:latest && ls -lh miniweather.tar\n",
    "!singularity build --force miniweather.sif docker-archive://miniweather.tar && rm -f miniweather.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourth and finally, run the MPI version of the code, for a single MPI rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!singularity exec miniweather.sif mpirun --allow-run-as-root -n 1 -mca btl_base_warn_component_unused 0 /opt/miniWeather/bin/miniWeather_mpi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is configured to run the \"injection\" case.  A narrow jet of fast and slightly cold wind is injected into a balanced, neutral atmosphere at rest from the left domain.\n",
    "This has nothing to do with atmospheric flows. It's just here for looks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh output.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have the correct python libraries installed. Also you might want netcdf tools intalled \n",
    "\n",
    "E.g. if the host is CentOS/RedHat\n",
    "```bash\n",
    "dnf install -y netcdf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install netCDF4 matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netcdf output {\n",
      "dimensions:\n",
      "\tt = UNLIMITED ; // (1 currently)\n",
      "\tx = 2000 ;\n",
      "\tz = 1000 ;\n",
      "variables:\n",
      "\tdouble t(t) ;\n",
      "\tdouble dens(t, z, x) ;\n",
      "\tdouble uwnd(t, z, x) ;\n",
      "\tdouble wwnd(t, z, x) ;\n",
      "\tdouble theta(t, z, x) ;\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!ncdump -h output.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f92c0829da0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import netCDF4\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f = netCDF4.Dataset('output.nc', 'r')\n",
    "theta = f.variables['theta']\n",
    "plt.imshow(theta[0,:,:], origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Summary\n",
    "\n",
    "In this lab, you have learned:\n",
    "\n",
    "1. How to build container images using Singularity.  Singularity container images are single \"flat\" files, making them easy to use at runtime.\n",
    "\n",
    "2. How to build container images using Docker.  Image layers are an important concept, enabling cached and multi-stage builds.  However, incorrect use of image layers can lead to unnecessarily large container images.\n",
    "\n",
    "3. HPC Container Maker is an open source tool that simplifies the specification of container images.  From a Python recipe, it can generate either a Dockerfile or a Singularity definition file.  Python is a more powerful language for expression container specifications, and the HPCCM building blocks separate the high level choice of what HPC software components to include in a container image from the low level complexities.\n",
    "\n",
    "You should now understand the benefits of building HPC container images using the workflow:\n",
    "\n",
    "1. Specify the content of container images with [HPC Container Maker](https://github.com/NVIDIA/hpc-container-maker)\n",
    "2. Build container images with Docker\n",
    "3. Convert the Docker images to Singularity images\n",
    "4. Use Singularity to run containers on your HPC system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Converting Docker Container Images with Singularity 2.x\n",
    "\n",
    "The `docker-daemon` endpoint was introduced in Singularity 3.0.  Fortunately there is a convenient [container to convert local Docker images into Singularity 2.x images](https://hub.docker.com/r/singularityware/docker2singularity) available on Docker Hub.\n",
    "\n",
    "For example, to convert the MPI Bandwidth container image to a Singularty 2.x `simg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo docker run -t --rm --cap-add SYS_ADMIN -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/output singularityware/docker2singularity mpi_bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Terminology\n",
    "\n",
    "- Container: a running instantiation of a container image\n",
    "\n",
    "- Container image: a standalone \"package\" of software that includes everything needed to run an application\n",
    "\n",
    "- Container runtime: a software framework to run and manage containers and container images.  Examples: Docker, Singularity.\n",
    "\n",
    "- Container registry: a server hosting container images for download (\"pulling\").  Examples: Docker Hub, Singularity Hub, NVIDIA GPU Cloud (NGC).\n",
    "\n",
    "## Appendix: Getting Your System Container Ready\n",
    "\n",
    "Docker and Singularity have been setup already for you in this lab environment. For more information on installing Singularity on your system, please see this brief [video](https://www.youtube.com/watch?v=iOLVqqHQsBU)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
